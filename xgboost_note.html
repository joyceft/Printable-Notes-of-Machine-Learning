<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">

<style>
body {
  font-family: Arial;
  font-size: 12px;
}

div {
  column-count: 2;
  column-gap : 10px;
  column-rule-style: solid;
  column-rule-color: white;
}

h1{
	background-color: #0E6655;
	color :#FFFFFF;
	text-align :center;
}
h2{
	text-align:left;
	font-weight: bold;
	color:#515A5A;
	}
ol > li{
	font-weight : bold;
	}
@media print {
    .pagebreak { page-break-before: always; } /* page-break-after works, as well */
}
</style>
</head>
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<body >
<h1>Extreme Gradient Boosting</h1>
<div>
<h2>XGBoost Advantage</h2>
<ol>
	<li>Regularization:</li>
	<ul>
		<li>Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.
		<li>In fact, XGBoost is also known as a ‘regularized boosting‘ technique.
	</ul>
	<li>Parallel Processing:</li>
	<ul>
		<li>XGBoost implements parallel processing and is blazingly faster as compared to GBM.</li>
		<li>But hang on, we know that boosting is a sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.</li>
		<li>XGBoost also supports implementation on Hadoop.</li>
	</ul>
	<li>High Flexibility:</li>
	<ul>
		<li>XGBoost allows users to define custom optimization objectives and evaluation criteria.</li>
		<li>This adds a whole new dimension to the model and there is no limit to what we can do.</li>
	</ul>
	<li>Handling Missing Values:</li>
	<ul>
		<li>XGBoost has an in-built routine to handle missing values.</li>
		<li>The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.</li>
	</ul>
	<li>Tree Pruning:</li>
	<ul>
		<li>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.</li>
		<li>XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.</li>
		<li>Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.</li>
	</ul>	
	<li>Built-in Cross-Validation:</li>
	<ul>
		<li>XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
		<li>This is unlike GBM where we have to run a grid-search and only a limited values can be tested.</li>
		</ul>
	<li>Continue on Existing Model:</li>
	<ul>
		<li>User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.</li>
		<li>GBM implementation of sklearn also has this feature so they are even on this point.</li>
		</ul>
</ol>
<h2>XGBoost Training Parameters</h2>
<p>There are 3 categories of XGBoost:
<ul>
	<li><b>General Parameters</b>: Guide the overall functioning</li>
	<li><b>Booster Parameters</b>: Guide the individual booster (tree/regression) at each step</li>
	<li><b>Learning Task Parameters</b>: Guide the optimization performed</li>
</ul>
<pre class="prettyprint"><code class="language-r">
  xgb.train(params = list(), data, nrounds, 
	watchlist = list(), obj = NULL,
	feval = NULL, verbose = 1, 
	print_every_n = 1L,
	early_stopping_rounds = NULL, 
	maximize = NULL, save_period = NULL,
	save_name = "xgboost.model", 
	xgb_model = NULL, callbacks = list(), ...)
	<br>
  xgboost(data = NULL, label = NULL,
	missing = NA, weight = NULL,
	params = list(), nrounds, verbose = 1,
	print_every_n = 1L,
	early_stopping_rounds = NULL, 
	maximize = NULL, save_period = NULL,
	save_name = "xgboost.model", 
	xgb_model = NULL, callbacks = list(), ...)
</code></pre>
<h3>General Parameters</h3>
<ol>
		<b><li>booster [default=gbtree]</li></b>
			Select the type of model to run at each iteration. It has 2 options:
			<ul>
				<li>gbtree: tree-based models</li>
				<li>gblinear: linear models</li>
			</ul>
		<b><li>silent [default=0]:</li></b>
			<ul>
				<li>Silent mode is activated is set to 1, i.e. no running messages will be printed.</li>
				<li>It’s generally good to keep it 0 as the messages might help in understanding the model.</li>
			</ul>
		<b><li>nthread [default to maximum number of threads available if not set]</li></b>
			<ul>
				<li>This is used for parallel processing and number of cores in the system should be entered</li>
				<li>If you wish to run on all cores, value should not be entered and algorithm will detect automatically</li>
			</ul>
</ol>

  <h3>Booster Parameters--tree</h3>
Here I only include some common parameters of XGBoost Model.
<ol>
		<li>eta [ default = 0.3]</li>
			Learning rate, smaller eta requires more nrounds
			<ul>
				<li>Makes the model more robust by shrinking the weights on each step</li>
				<li>Typical final values to be used: 0.01-0.2</li>
			</ul>
		<li>min_child_weight [default=1]</li>
		The minimum sum of weights of all observations required in a child.
			<ul>
				<li>This is similar to <b>min_child_leaf</b> in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.</li>
				<li>Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.</li>
				<li>Too high values can lead to under-fitting hence, it should be tuned using CV.</li>
			</ul>
		<li>max_depth [default=6]</li>
			<ul>
				<li>Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.</li>
				<li>Should be tuned using CV.</li>
				<li>Typical values: 3-10</li>
			</ul>
		<li>max_leaf_nodes</li>The maximum number of terminal nodes or leaves in a tree.
			<ul>
				<li>Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.</li>
				<li>If this is defined, GBM will ignore max_depth.</li>
			</ul>
		<li>gamma [default=0]</li>
		A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.
			<br>Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
		<li>subsample [default=1]</li>
		Denotes the fraction of observations to be randomly samples for each tree.
			<ul>
				<li>Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.</li>
				<li>Typical values: 0.5-1</li>
			</ul>			
		<li>colsample_bytree [default=1]</li>
		Denotes the fraction of columns to be randomly samples for each tree.
			<ul>
				<li> </li>
				<li>Typical values: 0.5-1</li>

			</ul>			
		<li>scale_pos_weight [default=1]</li>
		A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.			
</ol>
<h3>Learning Task Parameters--tree</h3>
These parameters are used to define the optimization objective the metric to be calculated at each step.. Here I only include some common parameters of XGBoost Model.
<ol>
		<li>objective [default=reg:linear]</li>
		This defines the loss function to be minimized. Mostly used values are:
			<ul>
				<li><b>binary:logistic</b> –logistic regression for binary classification, returns predicted probability (not class)</li>
				<li><b>multi:softmax</b> –multiclass classification using the softmax objective, returns predicted class (not probabilities)</li>
				For this objective, you need to set an additional <b>num_class</b> parameter define the number of unique classes.
				<li><b>multi:softprob</b> –same as softmax, but returns predicted probability of each data point belonging to each class.</li>
			</ul>
		<li>eval_metric [ default according to objective ]</li>
		The metric to be used for validation data.The default values are <b>rmse</b> for regression and <b>error</b> for classification. Typical values are:
			<ul>
				<li><b>rmse</b> – root mean square error</li>
				<li><b>mae</b> – mean absolute error</li>
				<li><b>logloss</b> – negative log-likelihood</li>
				<li><b>error</b> – Binary classification error rate (0.5 threshold)</li>
				<li><b>merror</b> – Multiclass classification error rate</li>
				<li><b>mlogloss</b> – Multiclass logloss</li>
				<li><b>auc</b> - Area under the curve</li>
			</ul>
		<li>seed [default=0]</li>
		The random number seed. Can be used for generating reproducible results and also for parameter tuning.		
</ol>
<h3>Model Implementation</h3>
<ol>
	<li>xgb</li> – this is the direct xgboost library. I will use a specific function “cv” from this library
	<li>XGBClassifier</li> – this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM.
</ol>
 <h3>Parameter Tuning</h3>
 There are many ways of tuning parameters, for XGBClassifier specifically, tuning <b>learning rate</b> and <b>n_estimator</b> together and other parameters together.
 Common Tuning methods:
 <ul>
	<li><b>RandomizedSearchCV</b></li> which randomly choose parameters from the param_distributions.The number of parameter settings that are tried is given by n_iter.
<pre class="prettyprint"><code class="language-python">
class sklearn.model_selection.
RandomizedSearchCV(
  estimator, param_distributions, n_iter=10, 
  scoring=None, n_jobs=None, iid='warn', 
  refit=True, cv='warn', verbose=0, 
  pre_dispatch='2*n_jobs', random_state=None,
  error_score='raise-deprecating', 
  return_train_score=False)
</code></pre>
	<li><b>GridSearchCV</b></li> which do bruce force of each parameter inside the param_grid. 
<pre class="prettyprint"><code class="language-python">	
class sklearn.model_selection.
GridSearchCV(
 estimator, param_grid, scoring=None, 
 n_jobs=None, iid='warn', refit=True,
 cv='warn', verbose=0, 
 pre_dispatch='2*n_jobs',
 error_score='raise-deprecating', 
 return_train_score=False)
</code></pre> 
 </ul>
 Common parameters descriptions in RandomizedSearchCV and GridSearchCV are:
 <ul>
	<li><b>estimator: estimator object.</b> the model you specify</li>
	<li><b>param_distributions/param_grid: dict</b> Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.</li>
	<li><b>n_iter : int, default=10</b> Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.</li>
	<li><b>scoring : string, callable, list/tuple, dict or None, default: None </b>A single string <a href = 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter'>custom evalution metric</a> or a callable to evaluate the predictions on the test set. Check <a href = 'https://scikit-learn.org/stable/modules/grid_search.html#multimetric-grid-search'>Example</a><br>If you have multiple metrics, also use <b>refit</b>. </li>
	<li><b>n_jobs: int or None, optional (default=None) </b>Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. </li>
	<li><b>iid : boolean, default=’warn’ </b>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds.  </li>
	<li><b>cv : int, cross-validation generator or an iterable, optional </b>Determines the cross-validation splitting strategy. Possible inputs for cv are: None(default cv = 3); integer(using StratifiedKFold) </li>
	<li><b>refit : boolean, string, or callable, default=True </b>Refit an estimator using the best found parameters on the whole dataset. </li>
	<li><b>random_state : int, RandomState instance or None, optional, default=None </b> </li>
 </ul>
 Common attributes are: 
 <ul>
	<li><b>cv_results_ : dict of numpy (masked) ndarrays </b> </li>
	<li><b>best_estimator_ : estimator or dict </b>Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if <i>refit</i>=False. </li>
	<li><b>best_score_ : float </b>Mean cross-validated score of the best_estimator. </li>
	<li><b>best_params_ : dict </b>Parameter setting that gave the best results on the hold out data. </li>
	<li><b>best_index_ : int </b>The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting. </li>
	<li><b>scorer_ : function or a dict </b>Scorer function used on the held out data to choose the best parameters for the model. </li>
	<li><b>n_splits_ : int </b>The number of cross-validation splits (folds/iterations). </li>
 </ul>
 Detailed documents, see <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html'> sklearn.model_selection.RandomizedSearchCV</a> and <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV'>sklearn.model_selection.GridSearchCV</a>
  </div>
  <div class = 'pagebreak'></div>
  <div>
  
</body>
</html>